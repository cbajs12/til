## Spark SQL
- 구조화된 데이터란 것은 스키마를 가진 모든 데이터를 말한다.
- 스파크 SQL은 파이썬, 자바 ,스칼라에서 DataFrame 추상화 클래스를 제공하는데, 이는 구조화된 데이터 셋을 다루는 작업을 간편하게 만들어 준다. DataFrame은 관계형 DB의 테이블과 유사한 개념이다.
- 다양한 구조적 포맷의 데이터를 읽고 쓸수 있다.
- 스파크 프로그램 내부에서나 표준 데이터베이스 연결을 제공하는 외부툴등에서 스파크 SQL을 통해 SQL로 데이터를 질의할 수 있다.

### 라이브러리 링크
- 스파크 SQL은 하둡의 SQL 엔진인 아파치 하이브를 포함할수 있다. 그러나 하이브 라이브러리를 포함시킨다고 해서 하이브 설치가 필요한 것은 아니다. 하이브 지원과 함께 스파크 SQL을 빌드하는 것이 좋다.
- HiveQL과 다른 하이브 기반 기능을 사용할 수 있는 HiveContext가 존재한다.
- SQLCOntext는 스파크 SQL의 기능들을 쓰지만 하이브에 의존하지 않는다.

### 어플리케이션에서 스파크 SQL 사용
- 스파크SQL을 사용하는 가장 강력한 방식은 스파크 어플리케이션 내부에서 사용하는 것이다.
- 쉽게 SQL을 써서 데이터를 적재하고 쿼리를 날릴수 있게 해주면서 동시에 각 프로그래밍 언어로 작성한 일반적인 코드와 같이 사용이 가능하다.
- 이렇게 사용하려면 HiveQL이나 SQLContext를 만들어야 한다. 이것은 질의 기능 및 스파크 SQL 데이터와 연동하기 위한 추가 기능들을 제공한다.

#### 사용
- 시작하기 위해선 HiveQL이나 SQLContext를 임포트해야 한다.
- 쿼리를 만들기 위해서는 HiveQL이나 SQLContext의 객체의 `sql()`을 사용하면 된다.

#### 데이터프레임
- 데이터를 읽어 들이는 것과 쿼리를 실행하는 것은 모두 결과로 데이터프레임을 되돌려 준다.
- 데이터프레임은 데이터베이스 테이블과 비슷하다.
- 데이터프레임은 RDD에 접근할수 있는 방법도 제공하므로(`rdd()`) 기존의 map()이나 filter()를 쓴 연산도 가능하다.
- 어떤 데이터프레임이든 임시 테이블로 등록하여 HiveContext.sql이나 SQLContext.sql로 질의를 날릴수 있다.
- 데이터프레임은 임시 테이블 등록이나 RDD 조작없이 직접 연산을 제공하는 여러개의 트랜스포메이션을 가지고 있다.(`show()`, `select()`, ...)
- 데이터프레임은 기본 타입뿐만 아니라 구조체 또는 타입들의 배열도 담을수 있으며, 타입 정의에 HiveQL문법을 사용한다.
- Row객체는 데이터프레임 내부에서 레코드를 의미하고, 단순히 필드들의 고정 길이 배열이다.

#### 캐싱
- 스파크SQL은 각 칼럼의 타입을 알고 있어 효율적으로 데이터를 저장할 수 있다.
- 전체 객체를 저장하는 방식보다는 메모리 효율적으로 캐싱하도록 확실히 하기 위해서는 `hiveCtx.cacheTable("tableNmae")`메소드라는 특수한 메소드를 써야 한다.
- 테이블을 캐싱할 때 스파크SQL은 데이터를 메모리에 칼럼 지향 포맷으로 저장한다.
- 캐시된 테이블은 메모리에 드라이버 프로그램이 실행되는 동안만 존재한다.
- HiveQL이나 SQL문장으로도 테이블 캐싱은 가능하다. 테이블을 캐시하거나 캐시에서 해제하려면 단순히 CACHE TABLE tableName 이나 UNCACHE TABLE tableNmae으로 가능하다.

### 데이터 불러오고 저장하기
- 스파크 SQL은 여러가지의 구조화된 데이터 소스를 기본적으로 지원하므로 복잡한 절차없이 데이터 소스로부터 Row 객체를 받아 올 수 있다.
- 데이터 소스들에 SQL로 쿼리를 날리고 필요한 필드만을 지정해 요청한다면 스파크SQL은 그필드의 데이터들만 탐색한다. 전체 데이터를 검색하지 않는다.

#### 아파치 하이브
- 데이터를 하이브에서 읽어올때 스파크SQL은 하이브가 지원하는 어떤 저장 타입도 모두 지원한다.

#### 데이터 소스
- 스파크SQL은 HBase나 엘라스틱 서치등의 데이터 소스와 연동할 수 있도록 데이터 소스 API를 지원한다.

#### JSON
- 동일한 스키마로 맞춰진 레코드들의 JSON파일을 갖고 있다면, 스파크SQL은 파일을 스캔해 스키마를 추정하고 필드에 이름으로 접근할수 있게 해준다.

#### RDD에서 가져오기
- RDD로부터 데이터프레임을 만들어 낼수 있다.
- HiveQL이나 SQLContext는 각 프로그래밍 언어에 맞는 함수를 제공하고있다. (`inferSchema()` - python)

#### JDBC/ODBC 서버
- 스파크SQL은 JDBC기능을 제공한다. 
- JDBC서버는 단독 스파크 드라이버 프로그램으로 실행되며 여러 클라이언트에 의해 공유될 수 있다.
- JDBC서버는 스파크가 하이브 지원을 포함해서 빌드되어야 한다.
- 캐시된 테이블은 JDBC서버를 쓰는 모든 클라이언트에서 공유된다.

#### 비라인으로 작업
- 비라인을 쓰면 테이블을 만들고 목록을 보고 쿼리를 날려보는 하이브QL 명령어들을 실행해 볼수 있다.

### 사용자 정의 함수
- 프로그래밍 언어로 직접 만든 로직의 함수를 등록하여 SQL내에서 호출할 수 있게 해준다.

#### 스파크 SQL UDF
- 스파크SQL은 프로그래밍 언어에서 함수만 전달해서 쉽게 UDF를 등록할수 있는 내장 메소드를 지원한다.

#### 하이브 UDF
- 스파크 SQL은 기존의 하이브 UDF를 쓸수도 있다. 표준 하이브 UDF들은 자동으로 불러와 사용할수 있다.

### 성능
- 스파크SQL은 메모리 기반 칼럼 지향 저장소를 사용하여, 캐시할때 적은 공간을 쓸뿐만 아니라 이후의 쿼리들이 데이터의 일부만 사용한다면 스파크SQL의 데이터 읽기 연산을 최소화 해주기도 한다.
- 조건절 하부 이동 기능은 스파크SQL이 쿼리의 일부분을 쿼리를 수행하는 엔진의 아랫단으로 보내준다. 
- 스파크의 일부 레코드만을 읽는다고 할때, 일반적인 방법은 전체 데이터셋을 읽은 다음 거기에 조건절을 실행하는 것이지만, 스파크SQL은 하부의 저장 시스템이 키 범위의 일부만 가져오거나 다른 제한 기능을 지원하면 제한 조건을 데이터 저장 시스템 레벨까지 내려보내어 수행함으로써 필요없는 데이터 읽기는 줄여준다.
- codegen은 스파크SQL이 각 쿼리를 실행 전에 자바 바이트 코드로 컴파일하게 한다. codegen 기능은 실행을 위한 특별한 코드를 생성하므로 오래 걸리는 쿼리들이나 반복적인 쿼리들을 상당히 빠르게 실행시키게 해준다.
그러나 매우 짧은 실시간성 쿼리에 이 설정을 적용하면 매 쿼리마다 실행하는 컴파일러 때문에 추가적인 부담이 더해질 수 있다.
- batchSize는 데이터프레임을 캐시할 때 스파크SQL은 RDD의 레코드들을 옵션에 주어진 배치 사이즈 만큼씩 묶어서 각 묶음을 압축한다. 너무 작게 사이즈를 잡으면 압축 효율이 떨어지고, 너무 큰 사이즈는 메모리에 큰 용량을 차지한다.
