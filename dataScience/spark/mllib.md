## MLlib
- 머신 러닝 기능들을 모아 놓은 스파크의 라이브러리이다.
- 클러스터에서 병렬로 돌 수 있게 설계된 MLlib은 다양한 학습 알고리즘을 갖고 있으며, 스파크의 모든 프로그래밍 언어로 사용할 수 있다.

### 개요
- 모든 데이터가 RDD로 표현된 분산 데이터 셋에서 다양한 알고리즘을 가져다 쓸 수 있게 해주는 것이다.
- 클러스터에서 잘 돌아가는 병렬 알고리즘들만 갖고 있다는 것이다. 고전 알고리즘에서 병렬 플랫폼에서 돌아가기 적당하지 않은 것은 포함되지 않았다.
- 병렬 클러스터에서 적합한 최신 알고리즘들 분산RF, k-means, ALS등을 포함했다.
- 이러한 선택은 MLlib이 각 알고리즘을 큰 데이터셋에 돌려보기 적합하도록 구성되어 있다는 것을 의미한다.
- 서로 다른 학습 모델들을 적용하려는 여러 개의 작은 데이터셋을 가지고 있다면, 단일 머신을 위한 학습 라이브러리(weka, scikit-learn)를 각 노드에 적용하고 스파크의 map()을 써서 병렬로 처리하는 것이 나을 것이다.
- 머신 러닝 파이프라인도 최적의 설정을 찾기 위해 다양한 설정에 따른 여러 개의 작은 데이터셋에 동일한 알고리즘을 적용해 보는 것이 일반적이다. 이는 스파크에서 다른 노드에 다른 알고리즘을 적용하도록 설정한 인자 리스트에 `paralleize()`를 적용하고, 다시 각 노드에 단일 머신 알고리즘을 적용하여 실현할 수 있다.
- MLlib자체는 하나의 모델을 위한 분산 대용량 데이터셋에 적용할 때 빛을 발한다.

### 머신러닝 기초
- 머신 러닝 알고리즘은 훈련 데이터와 종종 알고리즘의 동작 방식에 대한 수학적 목적의 극대화 등에 기반하여 예측이나 결정을 내리려고 시도한다. 학습해야 할 문제는 여러 가지의 타입이 존재하는데 분류, 회귀, 클러스터링등이 있고 모두 다른 목적을 가진다.
- 모든 학습 알고리즘은 학습 함수에 공급할 각 아이템의 특성을 정의할 필요가 있다.
- 대부분의 알고리즘은 수치적인 특성만을 정의하므로 대개 중요한 단계는 특성 추출과 이런 특성 벡터를 만들어 내기 위한 변형과정이다.
- 특성 벡터로 표현되고 나면 대부분의 머신 러닝 알고리즘은 이 벡터들에 기반해 잘 정의된 머신러닝 함수로 최적화한다.
- 알고리즘이 확습이 최종 결정한 것을 표현하는 모델을 리턴할 것이다.
- 대부분의 학습 알고리즘은 결과에 영향을 미치는 여러 가지 인자를 받아들이므로 실제 업부 환경의 파이프라인에서는 모델의 다양한 버전을 훈련시키고 각각을 평가한다.

### 데이터 타입
- vector
- LabeledPoint : 분류나 회귀같이 관리되는 학습 알고리즘을 위한 데이터의 라벨이 붙는 점이다. 특징 벡터와 라벨을 포함한다.
- Rating : 사용자에 의한 어떤 상품의 평가를 의믜
- 다양한 Model 클래스 : 각각의 모델은 훈련 알고리즘의 결과가 되며, 새 데이터 포인트에 대한 모델이나 RDD에 적용 하기 위한 `predict()`메소드를 보통 갖고 있다.

#### 벡터로 작업하기
- 벡터들은 고밀도와 저밀도 벡터 타입으로 만들어진다.
- 고밀도 벡터는 모든 데이터를 부동 소수의 배열에 저장한다. 특성 판별을 위한 많은 기술이 매우 저밀도의 벡터를 결과로 내놓는 편이므로 이 방식을 사용하는 것은 종종 키 최적화를 가져온다.
- 벡터를 생성하는 방식이 언어에 따라 약간씩 다르다.

### 알고리즘
#### TF-IDF
- 단어 빈도-역문서 빈도는 텍스트 문서에서 특징 벡터를 생성하는 간단한 방식이다.
- 각 문서의 각 단어에 대해 등장 빈도(TF)와 전체 문서군에서 얼마나 자주 단어가 나타나는지(IDF)이다. 이 값들의 곱은 특정 문서가 얼마나 그 단어와 연관 있는지를 보여준다.

#### 정량화
- 특징 벡터에서 각 요소의 척도를 고려하므로 특성들이 정량화 되었을때 잘 동작하며, 그를 위해 요소들이 동일하게 정량화 되어야 한다.(즉 모든 특성들이 평균 0의 값과 표준편자 1을 가지도록)

#### 정규화
- 벡터를 길이 1로 정규화 하는 것이 입력 데이터를 준비하는 데 도움이 된다.

#### word2Vec
- 많은 다운스트림 알고리즘에 데이터를 공급하는 데 쓰일수 있는 신경망 네트워크 기반 문자열의 특징을 판별하는 알고리즘이다.

### 통계
- Statistics.colStats(rdd) : 벡터의 RDD의 통계적인 요약을 계산하며, 최솟값, 최댓값, 평균등을 벡터들의 집합에 저장한다.
- Statistics.corr(rdd, method) : 벡터 RDD의 칼럼들 사이에 상관관계 행렬을 계산하며 피어슨이나 스피어만 상관관계 중 하나를 사용한다.
- Statistics.corr(rdd1, rdd2, method) : 부동 소수 값으로 이루어진 두 RDD간의 상관관계를 계산하며, 피어슨이나 스피어만 상관관계중 하나를 사용한다.
- Statistics.chiSqTest(rdd) : LabeledPoint 객체들의 RDD에서 모든 라벨 객체들에 대해 피어슨의 독립성 테스트를 한다. 테스트 통계, 각 특정의 자유성 정도 등을 포함한 ChiSqTestResult 객체들의 배열을 되돌려준다. 라벨과 특징값은 분류해야한다.

### 분류와 회귀
- 훈련 데이터를 사용한 특징 객체들로부터 변수를 예측하는 관리 학습의 일반적인 두가지 형태이다.
- 두 가지의 차이점은 변수의 형태이다. 분류에서는 그 변수가 이산적인데, 회귀는 변수의 값이 연속적이다.

#### 선형 회귀
- 가장 일반적인 방법중 하나 출력 변수를 특성들의 선형 조합 형태로 예측
- L1, L2의 정규화 회귀를 지원하며 라쏘와 능형 회귀라고 알려져 있다.

#### 로지틱스 회귀
- 양성과 음성 예제들 중에서 선형으로 구분할 수 있는 기준을 찾아내는 이단 분류 방식이다. 0과 1의 라벨을 사용
- 선형 회귀와 매우 유사한 알고리즘을 갖지만, 한 가지 차이점은 SGD 외에도 LBFGS(SGD에 비해 더 적은 횟수의 반복으로 점들을 모으는 뉴튼의 방식에 대한 접근)라는 알고리즘을 가지는 것이다.

#### 벡터 머신 지원
- 선형 구분 기준을 쓰는 또 다른 이중 분류 알고리즘
- 0과 1의 라벨을 사용

#### 나이브 베이즈
- 특징의 선형 함수에 기초해 각각의 점이 각 분류에 얼마나 잘 들어가 있는지 특정하는 다중 분류 알고리즘이다.
- 주로 TF-IDF특성에 대한 문서 분류에 쓰인다.
- MLlib은 다항 나이브 베이스를 구현했으며, 입력받는 특징으로 음수가 아닌 빈도들을 받아들인다.

#### 의사결정 트리와 랜덤 포레스트
- 의사결정 트리는 분류와 회귀 양쪽에 쓸수 있는 유연한 모델이다. 노드들을 갖고 있는 트리로 표현되며, 각 노드는 데이터 특징에 기반하여 yes/no기반 결정을 하거나 예측을 포함하는 잎 노드가 된다.
- 다수의 트리를 구축할수 있는 알고리즘 (스파크 1.2 추가)

### 클러스터링
- 높은 유사도를 가지는 클러스터로 그룹화가 이루어지는 자율 (unsupervised)학습 작업이다.
- 관리(supervised)학습들은 데이터에 라벨을 필요로 하지만, 클러스터링은 라벨 없는 데이터를 이해하기 위해 사용될 수 있다.
- 데이터 검사나 예외적인 것들을 발견하기 위해 쓰인다.

#### k-menas
- MLlib은 k-means와 함께 병렬 환경에서 더 나은 초기화를 제공하는 k-means|| 도 제공한다.
- k-means에서 가장 중요한 인자는 생성할 클러스트 개수 K이다.
- 실용적인 방법으로 여러 가지 값의 K로 평균적인 클러스터 간의 거리가 더이상 줄지 않을 때 까지 시도해 보는 것이다.

### 협업 필터링과 추천
- 협업 필터링은 다양하 상품에 대한 사용자의 평점과 상호 작용이 혼합하여 새로운 상품을 추천하는 추천 시스템을 위한 기술이다.

#### 교대 최소 제곱법(ALS)
- 클러스터들에서 정렬화가 우수한 인기 있는 협업 필터링 알고리즘인 교대 최소 제곱법을 구현하고 있다.
- ALS는 두 종류의 형태가 있는데 가시적인 평점과 잠재적인 평점이다. 가시적인 평점은 한 상품에 대한 각 사용자의 평점이 점수화되어야 하며 예측하는 평점도 점수가 된다.

### 차원 축소
- 고차원 공간에서 데이터 셋에 대한 점들이 주어지면 단순하게 분석할수 있도록 점들의 차원 단계를 낮춘다.

#### 주성분 분석
- 저차원 표현에서 데이터의 분산이 가장 커지도록 저차원 공간에 대한 매핑을 시도하고 유용성이 떨어지는 차원은 무시한다.
- 매핑 계산을 위해서는 정규화된 상관관계 행렬이 만들어지고 특이 벡터들과 행렬의 값들이 사용된다. 가장 큰 특이값들과 연관된 특이 벡터들은 원본 데이터의 분상의 큰 부분을 재구축하는데 사용된다.

#### 특이값 분해
- MLlib은 저수준의 특이값 분해(SVD)를 제공한다.
- 최상위 특이값들과 관련된 특이 벡터들을 저장 공간을 절약하고 노이즈를 줄여 주며 행렬의 하부 단계 구조를 복구할 수 있게 해주어 분해가 꼭 필요하다.

### 모델 평가
- 모델 평가는 종단 간의 머신 러닝 파이프라인에서 중요한 부분을 차지한다.
- 모델의 과적합이나 인자 설정등의 의한 방해가 존재할 수 있어 데이터 학습전에 모델을 테스트하여 최적 평가를 해봐야 한다.

### 팁과 성능 고려
#### 특성 준비
- 사용자가 입력하는 특성들이 잘 선별되었는냐에 따라 우수함이 결정된다.

#### 알고리즘 설정
- 정확도가 향상되는지 확인하기 위해 기본값보다 더 반복횟수를 올리는 것도 시도해야 한다.

#### 재사용을 위한 캐싱
- MLlib에 데이터를 넘기기 전에 입력이 되는 데이터셋에 대해 `cache()`를 사용하는것은 중요하다.

#### 값의 밀도 확인
- 특성 벡터의 값이 대부분 0이고 저밀도 포맷으로 저장되면 큰 데이터셋에 대해 처리 시간과 공간을 아낄수 있다.

#### 병렬화 수준
- 대부분의 알고리즘에 대해서 입력 RDD의 파티션 갯수는 최소 전체 코어 개수만큼은 되어야 병렬화를 최대로 활용할 수 있다.
- 너무 많은 파티션을 쓰면 통신 비용이 증가하게 된다.

### 파이프라인 API
- 머신러닝을 위해 파이프라인 개념에 기반한 새로운 고수준 API, Scikit-learn의 파이프라인 API와 유사하다.
- 데이터셋을 변환하는 알고리즘들의 모음이다.
- 파이프라인은 전체 데이터셋에 대한 일관된 표현 방식을 사용하는데 `SchemaRDD`이다.