## Gradient descent
- 최적화 문제들을 풀때 사용한다.
- gradient는 함수가 가장 빠르게 증가할 수 있는 방향을 나타낸다.
- 임의의 시작점을 잡은후, gradient를 계산하고, gradient의 방향(함수의 출력값이 가장 많이 증가하는 방향)으로 조금 이동하는 과정을 여러 번 반복하는것이다.
- 함수의 최솟값은 반대 방향으로 이동함으로써 구한다.

### 적용
- 임의의 시작점을 잡고, gradient가 아주 작아질 때까지 경사의 반대 방향으로 조금씩 이동한다.
- 적절한 이동거리를 정하려면, 첫째 이동거리를 고정하거나, 둘쨰 시간에 따라 이동 거리를 점차 줄이거나, 셋쨰 이동할 때마다 목적 함수를 최소화 하는 이동 거리로 정하는 방법이 존재한다.
- 세번째 방법은 계산비용이 크다는 단점이 있다.
- 경사 하강법을 이용하면 오류값을 최소화하는 파라미터를 찾을 수 있다.(`minimize_batch` : 반복문을 돌 때마다 데이터셋 전체를 살펴본다.)

### Stochastic gradient descent
- `minimize_batch`를 이용하면 반복문을 돌 때마다 데이터 전체에 대해 gradient값을 계산해야 하기 때문에 계산이 오래 걸린다.
- 대부분의 오류 함수는 더할 수 있는 속성을 가진다. 즉, 데이터 전체에 대한 오류값이 각각 데이터 포인트에 대한 오류값의 합과 같다.
- 한 번 반복문을 돌 때마다 데이터 포인트 한 개에 대한 gradient를 계산하는 SGD를 사용할 수 있다. SDG는 수렴할 때까지 전체 데이터셋을 반복적으로 사용한다.
- 한번 반복문을 돌 때마다 임의의 순서대로 데이터 포인트를 사용한다. 각 데이터 포인트에 대해 gradient를 계산하면 된다. 최적해 근방에서 함수값이 한동안 줄지 않으면 이동 거리를 줄이고 알고리즘을 종료한다.