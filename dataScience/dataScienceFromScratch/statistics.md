## Statistics
### 중심 경향성
- 대부분의 경우 평균을 사용
- 중앙값(median)도 필요할수 있는데, 데이터 개수가 홀수면 전체 데이터에서 가장 중앙에 있는것이 중앙값, 짝수라면 전체 데이터에서 가장 중앙에 있는 두 데이터 포인트의 평균을 의미
- 평균의 경우 n개의 데이터가 주어졌을때, 데이터 한개의 값이 작은수 e만큼 증가한다면, 평균은 e/n만큼 증가할것이다.
- 중앙값을 찾기 위해서는 주어진 데이터를 정렬해야한다.
- 평균은 이상치에 매우 민감한데, 이상치가 이해하려는 현상을 제대로 나타내고 있지 않은 데이터라면 잘못된 정보를 제공할수 있다.

### 산포도
- 데이터가 얼마나 퍼져있는지를 나타낸다. 0과 근접한 값이면 거의 펴져있지 않다는것이고 반대면 매우 퍼져있다는 것이다.
- 가장 큰 값과 작은 값의 차이를 나타내는 법윈느 산포도를 나타내는 가장 간단한 통계치
- `편차`는 값으로부터 평균을 뺀값이고, 평균으로부터 얼마 떨어져 있는가를 나타낸다.
- `평균 편차`는 편차들의 절대값의 평균을 구하는 것이다.
- `분산`은 편차 제곱의 평균이며, 자료가 흩어져 있는 정도를 측정하는 것이다. 평균 값과는 독립적으로 산정한다.
- 분산은 자료단위의 제곱을 사용하므로 제곱근을 통하여 자료단위를 맞춰준다 이것을 `표준편차`라고 한다.
```python
def de_mean(x):
   x_bar = mean(x)
   return [i - x_bar for i in x]
def variance(x):
   n = len(x)
   deviations = de_mean(x)  # 편차
   return sum_of_squares(deviations)/(n-1)
def standard_deviation(x):
   return math.sqrt(variance(x))
```

- 편차의 제곱의 평균계산시 n대신에 n-1을 사용한것은 편차의 제곱합을 n으로 나누면 bias때문에 모분산에 대한 추정값이 실제 모분산보다 작게 계산되는것을 보정해주기 위함이다.
- 표준편차 또한 이상치에 나쁜 데이터가 있다면 잘못된 정보를 줄수 있다. 안정적인 방법은 상위25%와 하위 25%에 해당되는 값의 차이를 계산하는 것이다.

### 상관관계
- `공분산`은 두변수가 각각의 평균에서 얼마나 멀리 있는지를 살펴본다.
```python
def covariance(x, y):
   n = len(x)
   return dot(de_mean(x), de_mean(y)/(n-1))
```

- 공분산이 양수이면, x가 커지면 y도 커진다는것이고, 공분산이 음수이면, x가 커지면 y는 작아진다는 것이다. 또한 0이면 이와 같은 관계가 존재하지 않는다는 것이다.
- `상관관계`는 공분산에서 각각의 표준편차를 나눠준것이다. 단위가 없으면 항상 -1과 1사이의 값을 갖는다.
```python
def correlation(x,y):
   st_x = standard_deviation(x)
   st_y = standard_deviation(y)
   if st_x > 0 st_y > 0:
      return covariance(x, y)/st_x/st_y
   else:
      return 0
```

### 심슨의 역설
- 데이터를 분석하다보면 혼재변수(confounding variables)가 누락되어 상관관계가 잘못 계산되는 `심슨의역설`을 직면하게 된다.
- 상관관계는 다른 모든것이 동일할때 두변수의 관계를 나타낸다.
- 심슨의 역설을 피하려면 데이터를 이해하고 변수에 영향을 주는 모든 요인을 확인하는 방법 밖에 없다.
- 상관관계가 0이라는 것은 두변수 사이에 선형적인 관계가 없다는 것을 의미하지만 다른 종류의 관계가 존재할수있다.
- 상관관계는 연관성이 얼마나 크고 작은지 설명해 주지않는다.
- 상관관계는 인과관계를 의미하지 않는다.
- 인과관계를 확인해 보는 방법중 데이터를 무작위로 선택해서 확인해 보는 방법이 있다. 사용자를 비슷한 조건과 성절의 두 그룹으로 나누고 한 그룹에만 다른 요인을 적용해본다면, 해당 요인과 결과의 인과관계를 확인해 볼수 있다.
